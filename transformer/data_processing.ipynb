{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denev6/practice/blob/main/transformer/data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt9LGLSXyZpq",
        "outputId": "dbeda9ac-6570-4ca0-f478-9fdcd50a4fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.8.15\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qXlXpiWjwG8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download([\"punkt\", 'wordnet', 'omw-1.4', 'averaged_perceptron_tagger'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SskIfXoWvyS_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLO55TxdfYuV"
      },
      "outputs": [],
      "source": [
        "def join_path(*args):\n",
        "    return os.path.join(\"/content/drive/MyDrive/DACON\", *args)\n",
        "\n",
        "TRAIN_CSV = join_path(\"data\", \"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxt4Qhjiq0cS"
      },
      "outputs": [],
      "source": [
        "train_csv = pd.read_csv(TRAIN_CSV)\n",
        "utter = train_csv[\"Utterance\"]\n",
        "train_csv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5xgJ1I9rCrX",
        "outputId": "477f519a-9b0a-412e-899b-44b6cad7de14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Special symbols]\n",
            " / % – … ” ’ ‘ , [ & ; : ?   — ( $ - . * ! ' ] \" “ \n"
          ]
        }
      ],
      "source": [
        "res = list()\n",
        "\n",
        "for sent in utter.tolist():\n",
        "    marks = re.findall(r\"\\W\", sent)\n",
        "    res += marks\n",
        "\n",
        "print(\"[Special symbols]\")\n",
        "print(f\" {' '.join(set(res))} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C03ZpbwxrpQK",
        "outputId": "73faea72-7c63-4823-c966-1c279dfb6bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Word count]\n",
            "1 2 3 4 5 ... 41 42 44 48 69 \n",
            "\n",
            "- Median: 6.00\n",
            "- Mean: 7.95\n"
          ]
        }
      ],
      "source": [
        "sents = utter.map(lambda sent: str(sent).split()).map(len).tolist()\n",
        "count = list(set(sents))\n",
        "count.sort()\n",
        "\n",
        "print(\"[Word count]\")\n",
        "for len_ in count[:5]:\n",
        "    print(len_, end=\" \")\n",
        "print(\"... \", end=\"\")\n",
        "for len_ in count[-5:]:\n",
        "    print(len_, end=\" \")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"- Median: {np.median(sents):.2f}\")\n",
        "print(f\"- Mean: {np.mean(sents):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftQ5vdDJq1ly"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97XA-GEZmc6s"
      },
      "outputs": [],
      "source": [
        "class LabelEncoder(object):\n",
        "    def __init__(self):\n",
        "        self._targets = [\n",
        "            \"neutral\",\n",
        "            \"joy\",\n",
        "            \"surprise\",\n",
        "            \"anger\",\n",
        "            \"sadness\",\n",
        "            \"disgust\",\n",
        "            \"fear\",\n",
        "        ]\n",
        "        self.num_classes = len(self._targets)\n",
        "\n",
        "    def encode(self, label):\n",
        "        return self._targets.index(label)\n",
        "\n",
        "    def decode(self, label):\n",
        "        return self._targets[label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEcqXnYgE4AM"
      },
      "outputs": [],
      "source": [
        "class DataProcessor(object):\n",
        "    def __init__(self, label_encoder):\n",
        "        self._twt_tokenizer = TweetTokenizer(\n",
        "            preserve_case=False, strip_handles=True, reduce_len=True\n",
        "        )\n",
        "        self._lemmatizer = WordNetLemmatizer()\n",
        "        self._label_encoder = label_encoder\n",
        "        self._regex_dict = {\n",
        "            r\"n\\'t\": \" not\",\n",
        "            r\"\\'re\": \" are\",\n",
        "            r\"\\'s\": \" is\",\n",
        "            r\"\\'d\": \" would\",\n",
        "            r\"\\'ll\": \" will\",\n",
        "            r\"\\'ve\": \" have\",\n",
        "            r\"\\'m\": \" am\",\n",
        "            r\"\\'em\": \" them\",\n",
        "            r\"y\\'\": \"you \",\n",
        "            r\" *-+ *\": \" \",\n",
        "            r\"[:\\$-\\'\\\",\\d]\": \"\"\n",
        "        }\n",
        "        self._char_dict = {\n",
        "            \"–\": \"-\",\n",
        "            \"—\": \"-\",\n",
        "            \"“\": '\"',\n",
        "            \"”\": '\"',\n",
        "            \"‘\": \"'\",\n",
        "            \"’\": \"'\",\n",
        "            \"…\": \"...\"\n",
        "        }\n",
        "\n",
        "    def process(self, data, has_label=False):\n",
        "        data = data.reset_index(drop=True)\n",
        "        data[\"Utterance\"] = data[\"Utterance\"].map(self._process)\n",
        "\n",
        "        if has_label:\n",
        "            data[\"Target\"] = data[\"Target\"].map(self._label_encoder.encode)\n",
        "            data = data.loc[:, [\"Utterance\", \"Target\"]]\n",
        "        else:\n",
        "            data = data.loc[:, \"Utterance\"]\n",
        "            data = data.to_frame()\n",
        "        return data\n",
        "\n",
        "    def _process(self, sentence):\n",
        "        sentence = self.__unify_char(sentence)\n",
        "        tokens = self._twt_tokenizer.tokenize(sentence)\n",
        "        tokens = self.__shorten_repeated_tokens(tokens)\n",
        "        tokens = self.__remove_period_only(tokens)\n",
        "        sentence = self.__sub_regex(tokens)\n",
        "        sentence = self.__lemmatize(sentence)\n",
        "        return sentence\n",
        "    \n",
        "    def __unify_char(self, sentence):\n",
        "        for char, new in self._char_dict.items():\n",
        "            sentence = sentence.replace(char, new)\n",
        "        return sentence\n",
        "\n",
        "    def __shorten_repeated_tokens(self, tokens):\n",
        "        for i, token in enumerate(tokens):\n",
        "            if \"-\" in token:\n",
        "                token = token.split(\"-\")\n",
        "                token = \"-\".join(dict.fromkeys(token))\n",
        "                tokens[i] = token\n",
        "        return tokens\n",
        "    \n",
        "    def __remove_period_only(self, tokens):\n",
        "        if \".\" in tokens:\n",
        "            tokens.remove(\".\")\n",
        "        return tokens\n",
        "\n",
        "    def __sub_regex(self, tokens):\n",
        "        sentence = self.__decode_tokens_with_apostrophe(tokens)\n",
        "        for regex, word in self._regex_dict.items():\n",
        "            sentence = re.sub(regex, word, sentence)\n",
        "        return sentence\n",
        "        \n",
        "    def __lemmatize(self, sentence):\n",
        "        tokens = sentence.split()\n",
        "        token_tags = pos_tag(tokens)\n",
        "        for i, token_tag in enumerate(token_tags):\n",
        "            token, tag = token_tag\n",
        "            if tag.startswith(\"VB\"):\n",
        "                tokens[i] = self._lemmatizer.lemmatize(token, pos=\"v\")\n",
        "            elif tag.startswith(\"N\"):\n",
        "                tokens[i] = self._lemmatizer.lemmatize(token, pos=\"n\")\n",
        "        sentence = \" \".join(tokens)\n",
        "        return sentence\n",
        "\n",
        "    def __decode_tokens_with_apostrophe(self, tokens):\n",
        "        sentence = \" \".join(tokens)\n",
        "        marks = re.findall(r\"\\s\\W\\s*\", sentence)\n",
        "        for mark in marks:\n",
        "            if mark.strip() == \"'\":\n",
        "                sentence = sentence.replace(mark, mark.strip())\n",
        "        return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1SgoQR6u35N"
      },
      "outputs": [],
      "source": [
        "class RoBERTaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data,\n",
        "        roberta_tokenizer,\n",
        "        max_length=512,\n",
        "        mode=None,\n",
        "    ):\n",
        "        self._roberta_tokenizer = roberta_tokenizer\n",
        "        self._max_length = max_length\n",
        "        self._mode = mode\n",
        "        self._dataset = data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self._dataset.loc[idx, \"Utterance\"]\n",
        "        inputs = self._roberta_tokenizer(\n",
        "            text,\n",
        "            max_length=self._max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"][0]\n",
        "        attention_mask = inputs[\"attention_mask\"][0]\n",
        "\n",
        "        if self._mode == \"train\":\n",
        "            y = self._dataset.loc[idx, \"Target\"]\n",
        "            return input_ids, attention_mask, y\n",
        "        else:\n",
        "            return input_ids, attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPDUHfujx2NN"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFZRDzdWkoTz",
        "outputId": "afe2a1a2-60b1-4c8d-e677-cb7e6bba3a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (7991, 5)\n",
            "Eval: (1998, 5)\n"
          ]
        }
      ],
      "source": [
        "df_train, df_val = train_test_split(\n",
        "    train_csv, test_size=0.2, shuffle=True, random_state=36\n",
        ")\n",
        "print(f\"Train: {df_train.shape}\")\n",
        "print(f\"Eval: {df_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zxN5atGQDpz"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "processor = DataProcessor(label_encoder)\n",
        "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(ARGS[\"model\"], truncation=True)\n",
        "\n",
        "df_train = processor.process(df_train, has_label=True)\n",
        "df_val = processor.process(df_val, has_label=True)\n",
        "\n",
        "train_set = RoBERTaDataset(\n",
        "    df_train,\n",
        "    roberta_tokenizer,\n",
        "    max_length=ARGS[\"max_len\"],\n",
        "    mode=\"train\",\n",
        ")\n",
        "val_set = RoBERTaDataset(\n",
        "    df_val,\n",
        "    roberta_tokenizer,\n",
        "    max_length=ARGS[\"max_len\"],\n",
        "    mode=\"train\",\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=ARGS[\"batch_size\"])\n",
        "val_dataloader = DataLoader(val_set, batch_size=ARGS[\"batch_size\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
